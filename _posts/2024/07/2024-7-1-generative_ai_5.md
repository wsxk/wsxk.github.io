---
layout: post
tags: [AI]
title: "generative-ai å­¦ä¹ ç¬”è®° â…¤"
author: wsxk
comments: true
date: 2024-7-1
---

- [å‰è¨€](#å‰è¨€)


## å‰è¨€<br>
è€è§„çŸ©ï¼Œè¯·çœ‹å®Œå‰é¢çš„ç« èŠ‚ï¼Œanywayå…¶å®ä½ ä¸çœ‹ä¹Ÿæ²¡ä»€ä¹ˆå¤§é—®é¢˜~<br>
ğŸ˜„<br>

## 15. Retrieval Augmented Generation (RAG) and Vector Databases<br>
`RAG`ä¸»è¦è§£å†³çš„æ˜¯**LLMè®­ç»ƒæ•°æ®é›†è€æ—§ï¼Œä¸”å»é™¤äº†å¤§é‡ç§äººæ•°æ®å’Œå…¬å¸äº§å“æ‰‹å†Œçš„é—®é¢˜**<br>
æœ¬è´¨é€»è¾‘æ˜¯æŠŠæœ€æ–°ææ–™ä½œä¸º`prompt`ä¹Ÿæ”¾å…¥ç”¨æˆ·è¾“å…¥ä¸­ï¼Œä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMï¼Œè®©å…¶ç»™å‡ºåˆç†ç­”æ¡ˆã€‚<br>
ä½¿ç”¨ragçš„å¥½å¤„å¦‚ä¸‹ï¼š<br>
- 1. ä¿¡æ¯æ›´ä¸°å¯Œï¼Œæä¾›æœ€æ–°æ•°æ®
- 2. åˆ©ç”¨ç»è¿‡éªŒè¯çš„æ•°æ®æ¥å‡å°‘ä¼ªé€ 
- 3. ç›¸æ¯”fine-tuneæ›´çœé’±

### 15.1 RAGè¿è¡Œæ­¥éª¤<br>
```
1. çŸ¥è¯†åº“åŸºç¡€ï¼š
æœ€æ–°æ•°æ®åº“/æ–‡æ¡£è¢«åˆ‡åˆ†æˆchunkï¼Œéšåå–‚å…¥embedding modelè½¬æ¢ä¸ºvectorsã€‚

2. ç”¨æˆ·æŸ¥è¯¢ï¼š
ç”¨æˆ·æŸ¥è¯¢æŸä¸ªé—®é¢˜ã€‚

3. retrievalï¼ˆæ£€ç´¢ï¼‰ï¼š
ç”¨æˆ·æŸ¥è¯¢é—®é¢˜åï¼Œè¿™ä¸ªé—®é¢˜ä¼šè¢«æ£€ç´¢ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºç›¸å…³ä¿¡æ¯ï¼Œå°†ç›¸å…³ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡å’Œç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ç»“åˆï¼Œå‘é€ç»™LLM

4. LLMç”Ÿæˆï¼š
LLMåŸºäºæ£€ç´¢çš„æ•°æ®å’Œç”¨æˆ·çš„æŸ¥è¯¢è¿›è¡Œåˆ†æï¼Œå¾—åˆ°å“åº”å¹¶è¿”è¿˜ç»™ç”¨æˆ·ã€‚
Augmented Generation: the LLM enhances its response based on the data retrieved. It allows the response generated to be not only based on pre-trained data but also relevant information from the added context. The retrieved data is used to augment the LLM's responses. The LLM then returns an answer to the user's question.
```
æ ¹æ®æ£€ç´¢åˆ°çš„æ•°æ®çš„ä½¿ç”¨ä¸åŒï¼ŒRAGåˆåˆ†æˆäº†2ä¸ªå˜ä½“ï¼š<br>
```
RAG-Sequence,RAG-Sequence åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶çš„æ­¥éª¤å¦‚ä¸‹ï¼š
1. æ£€ç´¢é˜¶æ®µï¼šé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„å¤šä¸ªæ–‡æ¡£ã€‚
2. ç”Ÿæˆé˜¶æ®µï¼šåŸºäºæ£€ç´¢åˆ°çš„æ¯ä¸ªæ–‡æ¡£ï¼Œç‹¬ç«‹ç”Ÿæˆä¸€ä¸ªå€™é€‰ç­”æ¡ˆã€‚
3. æœ€ç»ˆç­”æ¡ˆï¼šä»æ‰€æœ‰å€™é€‰ç­”æ¡ˆä¸­é€‰æ‹©ä¸€ä¸ªä½œä¸ºæœ€ç»ˆç­”æ¡ˆï¼Œæˆ–è€…é€šè¿‡æŸç§é›†æˆæ–¹æ³•ï¼ˆå¦‚æŠ•ç¥¨ï¼‰ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

RAG-Token,RAG-Token åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶çš„æ­¥éª¤å¦‚ä¸‹ï¼š
1. æ£€ç´¢é˜¶æ®µï¼šåŒæ ·åœ°ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„å¤šä¸ªæ–‡æ¡£ã€‚
2. ç”Ÿæˆé˜¶æ®µï¼šä¸ RAG-Sequence ä¸åŒçš„æ˜¯ï¼ŒRAG-Token åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶æ˜¯é€è¯ç”Ÿæˆçš„ã€‚å³åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œéƒ½ä¼šè€ƒè™‘æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚
3. æœ€ç»ˆç­”æ¡ˆï¼šé€è¯ç”Ÿæˆæ•´ä¸ªç­”æ¡ˆï¼Œæ¯ä¸ªè¯çš„ç”Ÿæˆéƒ½ç»¼åˆè€ƒè™‘äº†æ£€ç´¢åˆ°çš„æ‰€æœ‰æ–‡æ¡£çš„ä¿¡æ¯ã€‚
```
**RAG-Sequenceï¼šåŸºäºæ¯ä¸ªæ£€ç´¢æ–‡æ¡£ç”Ÿæˆå¤šä¸ªå®Œæ•´ç­”æ¡ˆï¼Œç„¶åé€‰æ‹©æˆ–é›†æˆè¿™äº›ç­”æ¡ˆã€‚**<br>
**RAG-Tokenï¼šé€è¯ç”Ÿæˆç­”æ¡ˆï¼Œæ¯ä¸ªè¯çš„ç”Ÿæˆéƒ½åˆ©ç”¨äº†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„ä¿¡æ¯ã€‚**<br>

æ¥ä¸‹æ¥ä¼šä»‹ç»å¦‚ä½•æ‰§è¡Œ1/3æ­¥éª¤çš„å®ç°<br>

### 15.2 åˆ›å»ºçŸ¥è¯†åº“(å‘é‡åº“)<br>
ä¸ä¼ ç»Ÿæ•°æ®åº“ä¸åŒï¼Œå‘é‡æ•°æ®åº“æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå­˜å‚¨ã€ç®¡ç†å’Œæœç´¢`embedding vector`çš„æ•°æ®åº“ã€‚å®ƒå­˜å‚¨æ–‡æ¡£çš„æ•°å­—è¡¨ç¤ºã€‚å°†æ•°æ®åˆ†è§£ä¸º`embeddings`ä½¿æˆ‘ä»¬çš„ AI ç³»ç»Ÿæ›´å®¹æ˜“ç†è§£å’Œå¤„ç†æ•°æ®ã€‚<br>
æˆ‘ä»¬å°†`embeddings`å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå› ä¸º LLM æ¥å—çš„`token`æ•°é‡æ˜¯æœ‰é™çš„ã€‚ç”±äºæ— æ³•å°†æ•´ä¸ª`embeddings`ä¼ é€’ç»™ LLMï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬åˆ†è§£æˆå—`chunk`ï¼Œå½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œæœ€åƒé—®é¢˜çš„`embeddings`å°†ä¸æç¤ºä¸€èµ·è¿”å›ã€‚åˆ†å—è¿˜å¯ä»¥é™ä½é€šè¿‡LLMçš„`token`æ•°é‡çš„æˆæœ¬ã€‚<br>
ä¸€äº›æµè¡Œçš„çŸ¢é‡æ•°æ®åº“åŒ…æ‹¬ Azure Cosmos DBã€Clarifyaiã€Pineconeã€Chromadbã€ScaNNã€Qdrant å’Œ DeepLakeã€‚<br>
åˆ†å—çš„ä»£ç å¦‚ä¸‹:<br>
```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

### 15.3 æ£€ç´¢<br>
æ£€ç´¢çš„æ ¸å¿ƒæ€è·¯æœ‰4ç§<br>
```
1. Keyword search - used for text searches

2. Semantic search - uses the semantic meaning of words

3. Vector search - converts documents from text to vector representations using embedding models. 
Retrieval will be done by querying the documents whose vector representations are closest to the user question.

4. Hybrid - a combination of both keyword and vector search.
```
è¿™é‡Œé¢çš„å…³é”®ç‚¹åœ¨äºå¦‚ä½•**è¡¡é‡å‘é‡ç›¸ä¼¼åº¦**ï¼Œç›®å‰å¸¸ç”¨çš„æ–¹æ³•æœ‰`ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§å‡ é‡Œå¾—è·ç¦»ã€ç‚¹ç§¯`<br>

ä¸ºæ•°æ®åº“çš„æ¯ä¸ªå‘é‡åˆ›å»ºç´¢å¼•çš„æ–¹æ³•å¦‚ä¸‹:<br>
```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

æŸ¥è¯¢æ•°æ®åº“åï¼Œæ‚¨å¯èƒ½éœ€è¦æŒ‰æœ€ç›¸å…³çš„é¡ºåºå¯¹ç»“æœè¿›è¡Œæ’åºã€‚
```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

æ€»ç»“:<br>
```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assiatant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```


## 16. open-source-models<br>
anywayï¼Œåœ¨huggingfaceä¸Šä½¿ç”¨ä¸€äº›å¼€æºæ¨¡å‹å°±å¯¹äº†ï¼Œçœé’±åˆçœäº‹~<br>
[https://huggingface.co/chat](https://huggingface.co/chat)

## 17. ai-agents<br>
AI ä»£ç†æ˜¯ç”Ÿæˆå¼ AI é¢†åŸŸä¸­ä¸€ä¸ªéå¸¸ä»¤äººå…´å¥‹çš„é¢†åŸŸã€‚è¿™ç§å…´å¥‹æœ‰æ—¶ä¼šå¸¦æ¥æœ¯è¯­åŠå…¶åº”ç”¨çš„æ··æ·†ã€‚ä¸ºäº†ä½¿äº‹æƒ…ç®€å•åŒ–å¹¶æ¶µç›–å¤§å¤šæ•°æ¶‰åŠ AI ä»£ç†çš„å·¥å…·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å®šä¹‰ï¼š<br>
**äººå·¥æ™ºèƒ½ä»£ç†é€šè¿‡è®©å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è®¿é—®çŠ¶æ€å’Œå·¥å…·æ¥æ‰§è¡Œä»»åŠ¡ã€‚**<br>
å…¶å¤§æ¦‚åŸç†å›¾å¦‚ä¸‹ï¼š<br>
![](https://raw.githubusercontent.com/wsxk/wsxk_pictures/main/2024-3-25/20240629225719.png)
å¼€æºé¡¹ç›®`LangChain agent`å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œå…¶å·¥ä½œåŸç†å¦‚ä¸‹ï¼š<br>
![](https://raw.githubusercontent.com/wsxk/wsxk_pictures/main/2024-3-25/20240629225555.png)