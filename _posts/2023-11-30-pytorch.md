---
layout: post
tags: [AI]
title: "pytorch learning"
date: 2023-11-30    
author: wsxk
comments: true
---

- [1. install](#1-install)
- [2. dir/help](#2-dirhelp)
- [3. pytorch教程](#3-pytorch教程)
  - [3.1 torch.utils.tensorboard](#31-torchutilstensorboard)
  - [3.2 torchvision.transforms](#32-torchvisiontransforms)
  - [3.3 torchvision.datasets](#33-torchvisiondatasets)
  - [3.4 torch.utils.data.DataLoader](#34-torchutilsdatadataloader)


## 1. install<br>
请看[https://wsxk.github.io/AI_env/](https://wsxk.github.io/AI_env/)<br>

## 2. dir/help<br>
**dir用于查看一个库中有哪些组件，help用于查看组件的具体用法**<br>
```python
import torch
if __name__ == '__main__':
    print(dir(torch))
    print((dir(torch.cuda)))
    print(help((torch.cuda.is_available)))
```
![](https://raw.githubusercontent.com/wsxk/wsxk_pictures/main/2023-7-6/20231128224945.png)

## 3. pytorch教程<br>
### 3.1 torch.utils.tensorboard<br>
`tensorboard主要是画图用的`<br>
```python
from torch.utils.tensorboard import SummaryWriter
from PIL import Image
import numpy as np

writer = SummaryWriter("logs")
image_path = "DataSet/hymenoptera_data/train/ants/0013035.jpg"
image_pil = Image.open(image_path)
image_array = np.array(image_pil)
writer.add_image("picture",image_array,1,dataformats="HWC") # tag y轴 x轴 通道（height width channel）

# for i in range(100):
#     writer.add_scalar("y=2x",2*i,i)  # y=2x曲线 y轴 x轴
writer.close()
```
运行完后在terminal执行`tensorboard --logdir=logs --port=6007`
即可在浏览器中查看，`--logdir`是你存放的结果的目录，即SummaryWriter填入的目录<br>

### 3.2 torchvision.transforms<br>
`transforms主要是对图片进行处理,即把图片转换成tensor(张量)`<br>
```python
from PIL import Image
from torchvision import transforms
from torch.utils.tensorboard import SummaryWriter

if __name__ == "__main__":
    img_path = "DataSet/hymenoptera_data/train/ants/0013035.jpg"
    img = Image.open(img_path)
    writer = SummaryWriter("logs")

    # tensor
    tensor_trans = transforms.ToTensor()
    img_tensor = tensor_trans(img)
    writer.add_image("test", img_tensor, 0)

    # normalize (input-mean)/standard
    tensor_normal = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    img_normal = tensor_normal.forward(img_tensor)
    writer.add_image("test", img_normal, 1)

    # Resize 改变图片大小
    tensor_resize = transforms.Resize((512, 512))
    img_resize = tensor_resize(img)
    img_resize = tensor_trans(img_resize)
    writer.add_image("test", img_resize, 2)

    # Compose 组合，流水线步骤
    tensor_compose = transforms.Compose([tensor_resize, tensor_trans, tensor_normal])
    img_compose = tensor_compose(img)
    writer.add_image("test", img_compose, 3)

    # RandomCrop： 随机裁剪
    tensor_crop = transforms.RandomCrop((100, 100))
    tensor_compose2 = transforms.Compose([tensor_trans, tensor_crop])
    for i in range(10):
        img_crop = tensor_compose2(img)
        writer.add_image("Crop", img_crop, i)

    writer.close()
```

### 3.3 torchvision.datasets<br>
`datasets主要是用于加载数据集,并且其自集成了一些数据集，下载时，你需要转换成美国服务器（目前试过cn服务器与jp服务器，没成功下载）`<br>
```python
from torchvision.datasets import CIFAR10
from torchvision import transforms
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(log_dir="logs")
dataset_transform = transforms.Compose([transforms.ToTensor()])
train_set = CIFAR10(root="./torchvision_dataset", train=True, transform=dataset_transform, download=True)
test_set = CIFAR10(root="./torchvision_dataset", train=False, transform=dataset_transform, download=True)

# print(train_set)
# print(train_set[0])
# train, target = train_set[0]
# print(train)
# print(train_set.classes[target])
# train.show()

for i in range(10):
    img, target = train_set[i]
    writer.add_image("train_set", img, i)
```

### 3.4 torch.utils.data.DataLoader<br>
`DataLoader主要是用于加载数据集，其可以自动分批次，shuffle等`<br>
```python
from torchvision.datasets import CIFAR10
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

transformers = transforms.Compose([transforms.ToTensor()])
test_dataset = CIFAR10("torchvision_dataset", train=False, transform=transformers, download=False)
# 数据集，每次取64个数据，取完数据后重新清洗数据集中的数据，只有一个线程，不舍弃最后几张数据
test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=False)
writer = SummaryWriter(log_dir="logs")

# img, target = test_dataset[0]
# print(img.shape)
# print(target)
step = 0
for epoch in range(2):
    step = 0
    for i in test_dataloader:
        imgs, targets = i
        writer.add_images("test {}".format(epoch), imgs, step)
        step += 1
writer.close()
```