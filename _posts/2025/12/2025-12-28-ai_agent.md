---
layout: post
tags: [AI]
title: "AI agent学习1： basic"
author: wsxk
date: 2025-12-28
comments: true
---

- [1. 为什么要学ai agent](#1-为什么要学ai-agent)
- [2. 初识智能体](#2-初识智能体)
  - [2.1 智能体演变过程](#21-智能体演变过程)
  - [2.2 大语言模型驱动的新范式](#22-大语言模型驱动的新范式)
  - [2.3 智能体类型](#23-智能体类型)
- [3. 智能体的构成与运行原理](#3-智能体的构成与运行原理)
  - [3.1 智能体的协作模式](#31-智能体的协作模式)
  - [3.2 workflow和agent的差异](#32-workflow和agent的差异)
- [4. 智能体发展历史追溯](#4-智能体发展历史追溯)
  - [4.1 符号主义](#41-符号主义)
  - [4.2 心智社会](#42-心智社会)
  - [4.3 学习范式](#43-学习范式)
    - [4.3.1 基于强化学习的智能体](#431-基于强化学习的智能体)
  - [4.4 LLM智能体](#44-llm智能体)
    - [4.4.1 基于大规模数据的预训练](#441-基于大规模数据的预训练)
    - [4.4.2 基于大语言模型的智能体](#442-基于大语言模型的智能体)
- [5. 大模型交互](#5-大模型交互)
  - [5.1  提示词工程](#51--提示词工程)
  - [5.2 文本分词](#52-文本分词)
  - [5.3 大语言模型的缩放法则与局限性](#53-大语言模型的缩放法则与局限性)
    - [5.3.1 模型幻觉](#531-模型幻觉)
- [references](#references)


# 1. 为什么要学ai agent<br>
1. LLM的能力已经十分强大，强大到足以影响我的工作和生活，以LLM为核心的agent，通过让LLM能够调度各种工具，让LLM具备跟现实世界“交互”的能力，大大扩展了LLM的能力<br>
2. 与时俱进一下，理解现在的ai agent的工作原理是什么，能够做到什么程度，如何应用到日常工作当中。<br>

总之就是进行一个学。<br>

# 2. 初识智能体<br>
在探索任何一个复杂概念时，我们最好从一个简洁的定义开始。在人工智能领域，**智能体被定义为任何能够通过传感器（Sensors）感知其所处环境（Environment），并自主地通过执行器（Actuators）采取行动（Action）以达成特定目标的实体。**<br>
## 2.1 智能体演变过程<br>
1. 最简单的**反射智能体（Simple Reflex Agent）**，如温度计。它们的决策核心由工程师明确设计的“条件-动作”规则构成。<br>
局限性: 完全依赖于当前的感知输入，不具备记忆或预测能力。如果环境的当前状态不足以作为决策的全部依据，智能体该怎么办？<br>
2. **基于模型的反射智能体（Model-Based Reflex Agent）**，这类智能体**拥有一个内部的世界模型（World Model）**，用于追踪和理解环境中那些无法被直接感知的方面。它试图回答：“世界现在是什么样子的？”。例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。<br>
局限性：智能体缺乏明确的目标。<br>
3. **基于目标的智能体（Goal-Based Agent）**。与前两者不同，它的行为不再是被动地对环境做出反应，而是主动地、有预见性地选择能够导向某个特定未来状态的行动。这类智能体需要回答的问题是：“我应该做什么才能达成目标？”。经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。<br>
局限性: 目标单一，世界往往是复杂的，我们不仅希望到达公司，还希望时间最短、路程最省油并且避开拥堵，需要权衡条件。<br>
4. **基于效用的智能体（Utility-Based Agent）**。它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。它需要回答一个更复杂的问题：“哪种行为能为我带来最满意的结果？”。<br>
局限性: 核心决策逻辑，无论是规则、模型还是效用函数，依然依赖于人类设计师的先验知识。<br>
5. **学习型智能体（Learning Agent）**。**强化学习（Reinforcement Learning, RL）是实现这一思想最具代表性的路径。**一个学习型智能体包含一个性能元件（即我们前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。想象一个学习下棋的 AI。它开始时可能只是随机落子，当它最终赢下一局时，系统会给予它一个正向的奖励。通过大量的自我对弈，学习元件会逐渐发现哪些棋路更有可能导向最终的胜利。AlphaGo Zero 是这一理念的一个里程碑式的成就。<br>

## 2.2 大语言模型驱动的新范式<br>
以**GPT（Generative Pre-trained Transformer）** 为代表的大语言模型的出现，正在显著改变智能体的构建方法与能力边界。由大语言模型驱动的 LLM 智能体，其核心决策机制与传统智能体存在本质区别，从而赋予了其一系列全新的特性。<br>
这种转变，可以从两者在核心引擎、知识来源、交互方式等多个维度的对比中清晰地看出，如下图所示。简而言之，传统智能体的能力源于工程师的显式编程与知识构建，其行为模式是确定且有边界的；而 LLM 智能体则通过在海量数据上的预训练，获得了隐式的世界模型与强大的涌现能力，使其能够以更灵活、更通用的方式应对复杂任务。<br>
![](https://raw.githubusercontent.com/wsxk/wsxk_pictures/main/2025-9-25/20251222202123.png)<br>
这种差异使得 LLM 智能体可以直接处理高层级、模糊且充满上下文信息的自然语言指令。<br>

## 2.3 智能体类型<br>
```
（1）基于内部决策架构的分类
依据智能体内部决策架构的复杂程度分类：简单的反应式智能体，到引入内部模型的模型式智能体，再到更具前瞻性的基于目标和基于效用的智能体。此外，学习能力则是一种可赋予上述所有类型的元能力，使其能通过经验自我改进。

（2）基于时间与反应性的分类
这个视角关注智能体是在接收到信息后立即行动，还是会经过深思熟虑的规划再行动。这揭示了智能体设计中一个核心权衡：追求速度的反应性（Reactivity）与追求最优解的规划性（Deliberation）之间的平衡

（3）基于知识表示的分类
符号主义AI（Symbolic AI）：
符号主义，常被称为传统人工智能，其核心信念是：智能源于对符号的逻辑操作。这里的符号是人类可读的实体（如词语、概念），操作则遵循严格的逻辑规则。
其主要优势在于透明和可解释。由于推理步骤明确，其决策过程可以被完整追溯，这在金融、医疗等高风险领域至关重要。然而，其“阿喀琉斯之踵”在于脆弱性：它依赖于一个完备的规则体系，但在充满模糊和例外的现实世界中，任何未被覆盖的新情况都可能导致系统失灵，这就是所谓的“知识获取瓶颈”。

亚符号主义 AI（Sub-symbolic AI）：
知识并非显式的规则，而是内隐地分布在一个由大量神经元组成的复杂网络中，是从海量数据中学习到的统计模式。神经网络和深度学习是其代表。
这种方法的强大之处在于其模式识别能力和对噪声数据的鲁棒性 。它能够轻松处理图像、声音等非结构化数据，这在符号主义 AI 看来是极其困难的任务。然而，这种强大的直觉能力也伴随着不透明性。亚符号主义系统通常被视为一个黑箱（Black Box）。

神经符号主义 AI（Neuro-Symbolic AI）：
融合两大范式的优点，创造出一个既能像神经网络一样从数据中学习，又能像符号系统一样进行逻辑推理的混合智能体。LLM是其优秀案例
```

# 3. 智能体的构成与运行原理<br>
要理解智能体的运作，我们必须先理解它所处的任务环境。在人工智能领域，通常使用PEAS 模型来精确描述一个任务环境，即分析其**性能度量(Performance)、环境(Environment)、执行器(Actuators)和传感器(Sensors) 。**<br>
智能体并非一次性完成任务，而是通过循环，一步步执行直到达到目的，其运行机制：<br>
![](https://raw.githubusercontent.com/wsxk/wsxk_pictures/main/2025-9-25/20251222211357.png)
这个循环包括以下几个阶段:<br>
```
1. 感知 (Perception)：这是循环的起点。智能体通过其传感器（例如，API 的监听端口、用户输入接口）接收来自环境的输入信息。这些信息，即观察 (Observation)，既可以是用户的初始指令，也可以是上一步行动所导致的环境状态变化反馈。

2. 思考 (Thought)：接收到观察信息后，智能体进入其核心决策阶段。对于 LLM 智能体而言，这通常是由大语言模型驱动的内部推理过程。如图所示，“思考”阶段可进一步细分为两个关键环节：
规划 (Planning)：智能体基于当前的观察和其内部记忆，更新对任务和环境的理解，并制定或调整一个行动
  计划。这可能涉及将复杂目标分解为一系列更具体的子任务。
  工具选择 (Tool Selection)：根据当前计划，智能体从其可用的工具库中，选择最适合执行下一步骤的工具，并确定调用该工具所需的具体参数。

3. 行动 (Action)：决策完成后，智能体通过其执行器（Actuators）执行具体的行动。这通常表现为调用一个选定的工具（如代码解释器、搜索引擎 API），从而对环境施加影响，意图改变环境的状态。
```
行动并非循环的终点。智能体的行动会引起**环境(Environment)**的**状态变化(State Change)**，环境随即会产生一个新的**观察 (Observation)** 作为结果反馈。这个新的观察又会在下一轮循环中被智能体的感知系统捕获，形成一个持续的“感知-思考-行动-观察”的闭环。智能体正是通过不断重复这一循环，逐步推进任务，从初始状态向目标状态演进。<br>

## 3.1 智能体的协作模式<br>
大体分为2种:一种是作为高效工具，深度融入我们的工作流；另一种则是作为自主的协作者，与其他智能体协作完成复杂目标。<br>
```
1. 作为开发者工具的智能体
在这种模式下，智能体被深度集成到开发者的工作流中，作为一种强大的辅助工具。它增强而非取代开发者的角色，通过自动化处理繁琐、重复的任务，让开发者能更专注于创造性的核心工作。这种人机协同的方式，极大地提升了软件开发的效率与质量。
市面上的有GitHubCopilot、Claude Code、Trae、Cursor

2. 作为自主协作者的智能体
与作为工具辅助人类不同，第二种交互模式将智能体的自动化程度提升到了一个全新的层次，自主协作者。在这种模式下，我们不再是手把手地指导 AI 完成每一步，而是将一个高层级的目标委托给它。智能体会像一个真正的项目成员一样，独立地进行规划、推理、执行和反思，直到最终交付成果。这种从助手到协作者的转变，使得 LLM 智能体更深的进入了大众的视野。它标志着我们与 AI 的关系从“命令-执行”演变为“目标-委托”。智能体不再是被动的工具，而是主动的目标追求者。
市面上的有 CrewAI、AutoGen、MetaGPT、LangGraph。虽然具体实现千差万别，但它们的架构范式大致可以归纳为几个主流方向：
2.1 单智能体自主循环：这是早期的典型范式，如 AgentGPT 所代表的模式。其核心是一个通用智能体通过“思考-规划-执行-反思”的闭环，不断进行自我提示和迭代，以完成一个开放式的高层级目标。

2.2 多智能体协作：这是当前最主流的探索方向，旨在通过模拟人类团队的协作模式来解决复杂问题。它又可细分为不同模式： 角色扮演式对话：如 CAMEL 框架，通过为两个智能体（例如，“程序员”和“产品经理”）设定明确的角色和沟通协议，让它们在一个结构化的对话中协同完成任务。 组织化工作流：如 MetaGPT 和 CrewAI，它们模拟一个分工明确的“虚拟团队”（如软件公司或咨询小组）。每个智能体都有预设的职责和工作流程（SOP），通过层级化或顺序化的方式协作，产出高质量的复杂成果（如完整的代码库或研究报告）。AutoGen 和 AgentScope 则提供了更灵活的对话模式，允许开发者自定义智能体间的复杂交互网络。

2.3 高级控制流架构：诸如 LangGraph 等框架，则更侧重于为智能体提供更强大的底层工程基础。它将智能体的执行过程建模为状态图（State Graph），从而能更灵活、更可靠地实现循环、分支、回溯以及人工介入等复杂流程。
```
## 3.2 workflow和agent的差异<br>
在理解了智能体作为“工具”和“协作者”两种模式后，我们有必要对 Workflow 和 Agent 的差异展开讨论，尽管它们都旨在实现任务自动化，但其底层逻辑、核心特征和适用场景却截然不同。<br>
简单来说，**Workflow 是让 AI 按部就班地执行指令，而 Agent 则是赋予 AI 自由度去自主达成目标。**<br>

# 4. 智能体发展历史追溯<br>
智能体发展可以概括为4个阶段，符号主义、心智社会、学习范式和LLM智能体。<br>
## 4.1 符号主义<br>
人工智能领域的早期探索，深受数理逻辑和计算机科学基本原理的影响。在那个时代，研究者们普遍持有一种信念：人类的智能，尤其是逻辑推理能力，可以被形式化的符号体系所捕捉和复现。这一核心思想催生了人工智能的第一个重要范式——符号主义（Symbolicism），也被称为“逻辑AI”或“传统AI”。<br>
在符号主义看来，智能行为的核心是基于一套明确规则对符号进行操作。因此，一个智能体可以被视为一个物理符号系统：它通过内部的符号来表示外部世界，并通过逻辑推理来规划行动。这个时代的智能体，其“智慧”完全来源于设计者预先编码的知识库和推理规则，而非通过自主学习获得。<br>

> 1. 物理符号系统假说
> > 符号主义时代的理论根据，是1976年由艾伦·纽厄尔（Allen Newell）和赫伯特·西蒙（Herbert A. Simon）共同提出的物理符号系统假说（PhysicalSymbol SystemHypothesis, PSSH），假说包含2个核心论断
> > > 1. 充分性论断：任何一个物理符号系统，都具备产生通用智能行为的充分手段。
> > > 2. 任何一个能够展现通用智能行为的系统，其本质必然是一个物理符号系统。

一言以蔽之，即**智能的本质，就是符号的计算与处理。**<br>
**专家系统、SHRDLU都是基于此开发的**<br>

## 4.2 心智社会<br>
尽管早期项目成就显著，但从20世纪80年代起，符号主义AI在从“微观世界”走向开放、复杂的现实世界时，遇到了其方法论固有的根本性难题。这些难题主要可归结为两大类：<br>
```
（1）常识知识与知识获取瓶颈
符号主义智能体的“智能”完全依赖于其知识库的质量和完备性。然而，如何构建一个能够支撑真实世界交互的知识库，被证明是一项极其艰巨的任务，主要体现在两个方面：

知识获取瓶颈（Knowledge Acquisition Bottleneck）：专家系统的知识需要由人类专家和知识工程师通过繁琐的访谈、提炼和编码过程来构建。这个过程成本高昂、耗时漫长，且难以规模化。更重要的是，人类专家的许多知识是内隐的、直觉性的，很难被清晰地表达为“IF-THEN”规则。试图将整个世界的知识都进行手工符号化，被认为是一项几乎不可能完成的任务。
常识问题（Common-sense Problem）：人类行为依赖于庞大的常识背景（例如，“水是湿的”、“绳子可以拉不能推”），但符号系统除非被明确编码，否则对此一无所知。为广阔、模糊的常识建立完备的知识库至今仍是重大挑战，Cyc项目[4]历经数十年努力，其成果和应用仍然非常有限。

（2）框架问题与系统脆弱性
除了知识层面的挑战，符号主义在处理动态变化的世界时也遇到了逻辑上的困境。

框架问题（Frame Problem）：在一个动态世界中，智能体执行一个动作后，如何高效判断哪些事物未发生改变是一个逻辑难题[5]。为每个动作显式地声明所有不变的状态，在计算上是不可行的，而人类却能毫不费力地忽略不相关的变化。
系统脆弱性（Brittleness）：符号系统完全依赖预设规则，导致其行为非常“脆弱”。一旦遇到规则之外的任何微小变化或新情况，系统便可能完全失灵，无法像人类一样灵活变通。SHRDLU的成功，也正是因为它运行在一个规则完备的封闭世界里，而真实世界充满了例外。
```

马文·明斯基（Marvin Minsky）不再将心智视为一个金字塔式的层级结构，而是将其看作一个扁平化的、充满了互动与协作的“社会”。<br>
在明斯基的理论框架中，智能体的定义与我们第一章讨论的现代智能体有所不同。这里的智能体指的是一个极其简单的、专门化的心智过程，它自身是“无心”的。例如，一个负责识别线条的`LINE-FINDER`智能体，或一个负责抓握的`GRASP`智能体。<br>
这些简单的智能体被组织起来，形成功能更强大的`机构（Agency）`。一个机构是一组协同工作的智能体，旨在完成一个更复杂的任务。例如，一个负责搭积木的`BUILD`机构，可能由`SEE、FIND、GET、PUT`等多个下层智能体或机构组成。它们之间通过去中心化的激活与抑制信号相互影响，形成动态的控制流。<br>
**涌现（Emergence）是理解心智社会理论的关键。复杂的、有目的性的智能行为，并非由某个高级智能体预先规划，而是从大量简单的底层智能体之间的局部交互中自发产生的。**<br>

## 4.3 学习范式<br>
前文探讨的“心智社会”理论，在哲学层面为群体智能和去中心化协作指明了方向，但实现路径尚不明确。与此同时，符号主义在应对真实世界复杂性时暴露的根本性挑战也表明仅靠预先编码的规则无法构建真正鲁棒的智能。<br>
这两条线索共同指向了一个问题：如果智能无法被完全设计，那么它是否可以被学习出来？<br>
这一设问开启了人工智能的“学习”时代。其核心目标不再是手动编码知识，而是构建能从经验和数据中自动获取知识与能力的系统。本节将追溯这一范式的演进历程：从联结主义奠定的学习基础，到强化学习实现的交互式学习，直至今日由大型语言模型驱动的现代智能体。<br>
作为对符号主义局限性的直接回应，**联结主义（Connectionism）**在20世纪80年代重新兴起。与符号主义自上而下、依赖明确逻辑规则的设计哲学不同，联结主义是一种自下而上的方法，其灵感来源于对生物大脑神经网络结构的模仿。它的核心思想可以概括为以下几点：<br>
```
1. 知识的分布式表示：
知识并非以明确的符号或规则形式存储在某个知识库中，而是以连接权重的形式，分布式地存储在大量简单的处理单元（即人工神经元）的连接之间。整个网络的连接模式本身就构成了知识。

2. 简单的处理单元：
每个神经元只执行非常简单的计算，如接收来自其他神经元的加权输入，通过一个激活函数进行处理，然后将结果输出给下一个神经元。

3. 通过学习调整权重：
系统的智能并非来自于设计者预先编写的复杂程序，而是来自于“学习”过程。系统通过接触大量样本，根据某种学习算法（如反向传播算法）自动、迭代地调整神经元之间的连接权重，从而使得整个网络的输出逐渐接近期望的目标。
```
在这种范式下，智能体不再是一个被动执行规则的逻辑推理机，而是一个能够通过经验自我优化的适应性系统。<br>
**联结主义的兴起，特别是深度学习在21世纪的成功，为智能体赋予了强大的感知和模式识别能力，使其能够直接从原始数据（如图像、声音、文本）中理解世界，这是符号主义时代难以想象的。然而，如何让智能体学会在与环境的动态交互中做出最优的序贯决策，则需要另一种学习范式的补充。**<br>

### 4.3.1 基于强化学习的智能体<br>
联结主义主要解决了感知问题（例如，“这张图片里有什么？”），但智能体更核心的任务是进行决策（例如，“在这种情况下，我应该做什么？”）。**强化学习（Reinforcement Learning, RL）正是专注于解决序贯决策问题的学习范式。**它并非直接从标注好的静态数据集中学习，而是通过智能体与环境的直接交互，在“试错”中学习如何最大化其长期收益。<br>
以AlphaGo为例，其核心的自我对弈学习过程便是强化学习的经典体现。在这个过程中，AlphaGo（智能体）通过观察棋盘的当前布局（环境状态），决定下一步棋的落子位置（行动）。一局棋结束后，根据胜负结果，它会收到一个明确的信号：赢了就是正向奖励，输了则是负向奖励。通过数百万次这样的自我对弈，AlphaGo不断调整其内部策略，逐渐学会了在何种棋局下选择何种行动，最有可能导向最终的胜利。这个过程完全是自主的，不依赖于人类棋谱的直接指导。<br>
强化学习的框架可以用几个核心要素来描述：<br>
```

智能体（Agent）：学习者和决策者。在AlphaGo的例子中，就是其决策程序。

环境（Environment）：智能体外部的一切，是智能体与之交互的对象。对AlphaGo而言，就是围棋的规则和对手。

状态（State, S）：对环境在某一时刻的特定描述，是智能体做出决策的依据。例如，棋盘上所有棋子的当前位置。

行动（Action, A）：智能体根据当前状态所能采取的操作。例如，在棋盘的某个合法位置上落下一子。

奖励（Reward, R）：环境在智能体执行一个行动后，反馈给智能体的一个标量信号，用于评价该行动在特定状态下的好坏。例如，在一局棋结束后，胜利获得+1的奖励，失败获得-1的奖励。
```

## 4.4 LLM智能体<br>
### 4.4.1 基于大规模数据的预训练<br>
强化学习赋予了智能体从交互中学习决策策略的能力，但这通常需要海量的、针对特定任务的交互数据，导致智能体在学习之初缺乏先验知识，需要从零开始构建对任务的理解。无论是符号主义试图手动编码的常识，还是人类在决策时所依赖的背景知识，在RL智能体中都是缺失的。**如何让智能体在开始学习具体任务前，就先具备对世界的广泛理解？这一问题的解决方案，最终在自然语言处理（Natural Language Processing, NLP）领域中浮现，其核心便是基于大规模数据的预训练（Pre-training）。**<br>
在预训练范式出现之前，传统的自然语言处理模型通常是为单一特定任务（如情感分析、机器翻译）在专门标注的中小规模数据集上从零开始独立训练的。这种模式导致了几个问题：模型的知识面狭窄，难以将在一个任务中学到的知识泛化到另一个任务，并且每一个新任务都需要耗费大量的人力去标注数据。**预训练与微调（Pre-training, Fine-tuning）范式**的提出彻底改变了这一现状。其核心思想分为两步：<br>

```
1. 预训练阶段：
首先在一个包含互联网级别海量文本数据的通用语料库上，通过自监督学习（Self-supervised Learning）的方式训练一个超大规模的神经网络模型。
这个阶段的目标不是完成任何特定任务，而是学习语言本身内在的规律、语法结构、事实知识以及上下文逻辑。最常见的目标是“预测下一个词”。

2. 微调阶段：
完成预训练后，这个模型就已经学习到了和数据集有关的丰富知识。
之后，针对特定的下游任务，只需使用少量该任务的标注数据对模型进行微调，即可让模型适应对应任务。
```
通过在数万亿级别的文本上进行预训练，大型语言模型的神经网络权重实际上已经构建了一个关于世界知识的、高度压缩的隐式模型。它以一种全新的方式，解决了符号主义时代最棘手的“知识获取瓶颈”问题。**更令人惊讶的是，当模型的规模（参数量、数据量、计算量）跨越某个阈值后，它们开始展现出未被直接训练的、预料之外的涌现能力（Emergent Abilities）**，例如：<br>
```
1. 上下文学习（In-context Learning）：
无需调整模型权重，仅在输入中提供几个示例（Few-shot）甚至零个示例（Zero-shot），模型就能理解并完成新的任务。

2. 思维链（Chain-of-Thought）推理：
通过引导模型在回答复杂问题前，先输出一步步的推理过程，可以显著提升其在逻辑、算术和常识推理任务上的准确性。
```
### 4.4.2 基于大语言模型的智能体<br>
随着大型语言模型技术的飞速发展，以LLM为核心的智能体已成为人工智能领域的新范式。它不仅能够理解和生成人类语言，更重要的是，能够通过与环境的交互，自主地感知、规划、决策和执行任务。<br>
LLM驱动的智能体通过一个由多个模块协同工作的、持续迭代的闭环流程来完成任务。具体步骤如下：<br>
```
1. 感知 (Perception) ：
流程始于感知模块 (Perception Module)。它通过传感器从外部环境 (Environment) 接收原始输入，形成观察 (Observation)。这些观察信息（如用户指令、API返回的数据或环境状态的变化）是智能体决策的起点，处理后将被传递给思考阶段。

2. 思考 (Thought) ：
这是智能体的认知核心，对应图中的规划模块 (Planning Module) 和大型语言模型 (LLM) 的协同工作。
  2.1 规划与分解：首先，规划模块接收观察信息，进行高级策略制定。它通过反思 (Reflection) 和自我批判 (Self-criticism) 等机制，将宏观目标分解为更具体、可执行的步骤。
  2.2 推理与决策：随后，作为中枢的LLM 接收来自规划模块的指令，并与记忆模块 (Memory) 交互以整合历史信息。LLM进行深度推理，最终决策出下一步要执行的具体操作，这通常表现为一个工具调用 (Tool Call)。

3. 行动 (Action) ：
决策完成后，便进入行动阶段，由执行模块 (Execution Module) 负责。LLM生成的工具调用指令被发送到执行模块。该模块解析指令，从工具箱 (Tool Use) 中选择并调用合适的工具（如代码执行器、搜索引擎、API等）来与环境交互或执行任务。这个与环境的实际交互就是智能体的行动 (Action)。

4. 观察 (Observation) 与循环 ：
行动会改变环境的状态，并产生结果。
工具执行后会返回一个工具结果 (Tool Result) 给LLM，这构成了对行动效果的直接反馈。同时，智能体的行动改变了环境，从而产生了一个全新的环境状态。
这个“工具结果”和“新的环境状态”共同构成了一轮全新的观察 (Observation)。这个新的观察会被感知模块再次捕获，同时LLM会根据行动结果更新记忆 (Memory Update)，从而启动下一轮“感知-思考-行动”的循环。
这种模块化的协同机制与持续的迭代循环，构成了LLM驱动智能体解决复杂问题的核心工作流
```

# 5. 大模型交互<br>
## 5.1  提示词工程<br>
如果我们把大语言模型比作一个能力极强的“大脑”，那么**提示 (Prompt)** 就是我们与这个“大脑”沟通的语言。提示工程，就是研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复。对于构建智能体而言，一个精心设计的提示能让智能体之间协作分工变得高效。<br>
```
（1）模型采样参数
在使用大模型时，你会经常看到类似Temperature这类的可配置参数，其本质是通过调整模型对 “概率分布” 的采样策略，让输出匹配具体场景需求，配置合适的参数可以提升Agent在特定场景的性能。

低温度（0 ⩽ Temperature < 0.3）：输出更 “精准、确定”。
适用场景： 事实性任务：如问答、数据计算、代码生成； 严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。

中温度（0.3 ⩽ Temperature < 0.7）：输出 “平衡、自然”。
适用场景： 日常对话：如客服交互、聊天机器人； 常规创作：如邮件撰写、产品文案、简单故事创作。

高温度（0.7 ⩽ Temperature < 2）：输出 “创新、发散”。
适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发； 发散性思考。

Top-k ：其原理是将所有 token 按概率从高到低排序，取排名前 k 个的 token 组成 “候选集”，随后对筛选出的 k 个 token 的概率进行 “归一化”

Top-p ：其原理是将所有 token 按概率从高到低排序，从排序后的第一个 token 开始，逐步累加概率，直到累积和首次达到或超过阈值时，停止积累token

```
**如果上述三者都被采用，那么按照温度调整→Top-k→Top-p的顺序执行。温度调整整体分布的陡峭程度，Top-k 会先保留概率最高的 k 个候选，然后 Top-p 会从 Top-k 的结果中选取累积概率≥p 的最小集合作为最终的候选集。**<br>

```
（2）零样本、单样本与少样本提示
零样本提示 (Zero-shot Prompting) 这指的是我们不给模型任何示例，直接让它根据指令完成任务。这得益于模型在海量数据上预训练后获得的强大泛化能力。

单样本提示 (One-shot Prompting) 我们给模型提供一个完整的示例，向它展示任务的格式和期望的输出风格。

少样本提示 (Few-shot Prompting) 我们提供多个示例，这能让模型更准确地理解任务的细节、边界和细微差别，从而获得更好的性能。


（3）指令调优的影响
指令调优 (Instruction Tuning) 是一种微调技术，它使用大量“指令-回答”格式的数据对预训练模型进行进一步的训练。
经过指令调优后，模型能更好地理解并遵循用户的指令。我们今天日常工作学习中使用的所有模型（如 
ChatGPT, DeepSeek, Qwen）都是其模型家族中经过指令调优过的模型。

（4）基础提示技巧
角色扮演 (Role-playing) 通过赋予模型一个特定的角色，我们可以引导它的回答风格、语气和知识范围，使其输出更符合特定场景的需求。
上下文示例 (In-context Example) 这与少样本提示的思想一致，通过在提示中提供清晰的输入输出示例，来“教会”模型如何处理我们的请求，尤其是在处理复杂格式或特定风格的任务时非常有效。

（5）思维链
对于需要逻辑推理、计算或多步骤思考的复杂问题，直接让模型给出答案往往容易出错。思维链 (Chain-of-Thought, CoT) 是一种强大的提示技巧，它通过引导模型“一步一步地思考”，提升了模型在复杂任务上的推理能力。
实现 CoT 的关键，是在提示中加入一句简单的引导语，如“请逐步思考”或“Let's think step by step”。
```
## 5.2 文本分词<br>
我们知道，计算机本质上只能理解数字。因此，在将自然语言文本喂给大语言模型之前，必须先将其转换成模型能够处理的数字格式。这个将文本序列转换为数字序列的过程，就叫做**分词 (Tokenization)** 。<br>
**分词器 (Tokenizer) 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为词元(Token)**。<br>
理解分词算法的细节并非目的，但作为智能体的开发者，理解分词器的实际影响是重要，这直接关系到智能体的性能、成本和稳定性：<br>
```
1. 上下文窗口限制：
模型的上下文窗口（如 8K, 128K）是以 Token 数量计算的，而不是字符数或单词数。同样一段话，在不同语言（如中英文）或不同分词器下，Token 数量可能相差巨大。精确管理输入长度、避免超出上下文限制是构建长时记忆智能体的基础。

2. API 成本：
大多数模型 API 都是按 Token 数量计费的。了解你的文本会被如何分词，是预估和控制智能体运行成本的关键一步。

3. 模型表现的异常：
有时模型的奇怪表现根源在于分词。
例如，模型可能很擅长计算 2 + 2，但对于 2+2（没有空格）就可能出错，因为后者可能被分词器视为一个独立的、不常见的词元。
同样，一个词因为首字母大小写不同，也可能被切分成完全不同的 Token 序列，从而影响模型的理解。在设计提示词和解析模型输出时，考虑到这些“陷阱”有助于提升智能体的鲁棒性。
```

## 5.3 大语言模型的缩放法则与局限性<br>
**缩放法则（Scaling Laws）**是近年来大语言模型领域最重要的发现之一。它揭示了模型性能与模型参数量、训练数据量以及计算资源之间存在着可预测的幂律关系。这一发现为大语言模型的持续发展提供了理论指导，阐明了增加资源投入能够系统性提升模型性能的底层逻辑。<br>
研究发现，在对数-对数坐标系下，模型的性能（通常用损失 Loss 来衡量）与参数量、数据量和计算量这三个因素都呈现出平滑的幂律关系。简单来说，只要我们持续、按比例地增加这三个要素，模型的性能就会可预测地、平滑地提升，而不会出现明显的瓶颈。这一发现为大模型的设计和训练提供了清晰的指导：在资源允许的范围内，尽可能地扩大模型规模和训练数据量。<br>
早期的研究更侧重于增加模型参数量，但 DeepMind 在 2022 年提出的“Chinchilla 定律”对此进行了重要修正。该定律指出，**在给定的计算预算下，为了达到最优性能，模型参数量和训练数据量之间存在一个最优配比**。具体来说，最优的模型应该比之前普遍认为的要小，但需要用多得多的数据进行训练。例如，一个 700 亿参数的 Chinchilla 模型，由于使用了比 GPT-3（1750 亿参数）多 4 倍的数据进行训练，其性能反而超越了后者。这一发现纠正了“越大越好”的片面认知，强调了数据效率的重要性，并指导了后续许多高效大模型（如 Llama 系列）的设计。<br>
缩放法则最令人惊奇的产物是“能力的涌现”。所谓能力涌现，是指当模型规模达到一定阈值后，会突然展现出在小规模模型中完全不存在或表现不佳的全新能力。例如，链式思考 (Chain-of-Thought) 、指令遵循 (Instruction Following) 、多步推理、代码生成等能力，都是在模型参数量达到数百亿甚至千亿级别后才显著出现的。这种现象表明，大语言模型不仅仅是简单地记忆和复述，它们在学习过程中可能形成了某种更深层次的抽象和推理能力。对于智能体开发者而言，能力的涌现意味着选择一个足够大规模的模型，是实现复杂自主决策和规划能力的前提。<br>

### 5.3.1 模型幻觉<br>
当然，大语言模型也不是都是好处，其缺点也很明显:<br>
**模型幻觉（Hallucination）通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。**<br>
```
1. 事实性幻觉 (Factual Hallucinations) ： 模型生成与现实世界事实不符的信息。

2. 忠实性幻觉 (Faithfulness Hallucinations) ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。

3. 内在幻觉 (Intrinsic Hallucinations) ： 模型生成的内容与输入信息直接矛盾。
```
为了提高大语言模型的可靠性，研究人员和开发者正在积极探索多种检测和缓解幻觉的方法：
```
1. 数据层面： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式，从源头减少幻觉。
2. 模型层面： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。
3. 推理与生成层面：
  3.1 检索增强生成 (Retrieval-Augmented Generation, RAG) ： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。
  3.2 多步推理与验证： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。
4. 引入外部工具： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。
```


# references<br>
[https://datawhalechina.github.io/hello-agents/](https://datawhalechina.github.io/hello-agents)
写的挺好~<br>


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-C22S5YSYL7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-C22S5YSYL7');
</script>